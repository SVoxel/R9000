From bed567d50eb092e820722c83b77680fa852b74fb Mon Sep 17 00:00:00 2001
From: Ben Menchaca <ben.menchaca@qca.qualcomm.com>
Date: Fri, 7 Jun 2013 18:30:32 -0500
Subject: [netdev] generic skb_recycling

Defines a simplistic RX-to-RX skb recycling scheme for bridging and
routing workloads.  This version is designed to work with default RX
buffers up to 2KB in size.  We can certainly do pools of power-of-2
sizes, should the need arise.

The downside of this approach is for skb allocations that are
significantly smaller than 2K; these become 2K allocations.  However,
this is relatively rare for the two functions we have hooked.

We have used a per-CPU list for the SKBs, and we disable preemption
while accessing the list.

Overall, the performance gain on the wired-to-wireless bridging
workload was around 16%.

Signed-off-by: Ben Menchaca <ben.menchaca@qca.qualcomm.com>
---
 net/core/skbuff.c | 149 +++++++++++++++++++++++++++++++++++++++++++++++++++---
 1 file changed, 143 insertions(+), 6 deletions(-)

Index: linux-3.3.8/net/core/skbuff.c
===================================================================
--- linux-3.3.8.orig/net/core/skbuff.c	2013-08-15 09:10:00.822362163 -0500
+++ linux-3.3.8/net/core/skbuff.c	2013-08-23 09:53:59.115388005 -0500
@@ -59,6 +59,7 @@
 #include <linux/errqueue.h>
 #include <linux/prefetch.h>
 #include <linux/if.h>
+#include <linux/cpu.h>
 
 #include <net/protocol.h>
 #include <net/dst.h>
@@ -74,6 +75,9 @@
 
 static struct kmem_cache *skbuff_head_cache __read_mostly;
 static struct kmem_cache *skbuff_fclone_cache __read_mostly;
+static DEFINE_PER_CPU(struct sk_buff_head, recycle_list);
+#define SKB_RECYCLE_SIZE	2048
+#define SKB_RECYCLE_MAX_SKBS	2048
 
 static void sock_pipe_buf_release(struct pipe_inode_info *pipe,
 				  struct pipe_buffer *buf)
@@ -310,13 +314,45 @@
 struct sk_buff *__netdev_alloc_skb(struct net_device *dev,
 		unsigned int length, gfp_t gfp_mask)
 {
+	unsigned int len;
 	struct sk_buff *skb;
 
-	skb = __alloc_skb(length + NET_SKB_PAD, gfp_mask, 0, NUMA_NO_NODE);
-	if (likely(skb)) {
-		skb_reserve(skb, NET_SKB_PAD);
-		skb->dev = dev;
+	/*
+	 * Is this a request for an skb that we might be able to pull
+	 * from the recycling list?
+	 */
+	if (likely(length <= SKB_RECYCLE_SIZE)) {
+		unsigned long flags;
+		struct sk_buff_head *h = &get_cpu_var(recycle_list);
+		local_irq_save(flags);
+		skb = __skb_dequeue(h);
+		local_irq_restore(flags);
+		put_cpu_var(recycle_list);
+		if (likely(skb)) {
+			struct skb_shared_info *s = skb_shinfo(skb);
+			memset(s, 0, offsetof(struct skb_shared_info, dataref));
+			atomic_set(&s->dataref, 1);
+			memset(skb, 0, offsetof(struct sk_buff, tail));
+			skb->data = skb->head;
+			skb_reset_tail_pointer(skb);
+#ifdef NET_SKBUFF_DATA_USES_OFFSET
+			skb->mac_header = ~0U;
+#endif
+			skb_reserve(skb, NET_SKB_PAD);
+			skb->dev = dev;
+			return skb;
+		}
 	}
+
+	len = SKB_RECYCLE_SIZE;
+	if (unlikely(length > SKB_RECYCLE_SIZE))
+		len = length;
+	skb = __alloc_skb(len + NET_SKB_PAD, gfp_mask, 0, NUMA_NO_NODE);
+	if (unlikely(skb == NULL))
+		return NULL;
+
+	skb_reserve(skb, NET_SKB_PAD);
+	skb->dev = dev;
 	return skb;
 }
 EXPORT_SYMBOL(__netdev_alloc_skb);
@@ -361,11 +397,37 @@
  */
 struct sk_buff *dev_alloc_skb(unsigned int length)
 {
+	unsigned int len;
+
+	if (likely(length <= SKB_RECYCLE_SIZE)) {
+		unsigned long flags;
+		struct sk_buff *skb;
+		struct sk_buff_head *h = &get_cpu_var(recycle_list);
+		local_irq_save(flags);
+		skb = __skb_dequeue(h);
+		local_irq_restore(flags);
+		put_cpu_var(recycle_list);
+		if (likely(skb)) {
+			struct skb_shared_info *s = skb_shinfo(skb);
+			memset(s, 0, offsetof(struct skb_shared_info, dataref));
+			atomic_set(&s->dataref, 1);
+			memset(skb, 0, offsetof(struct sk_buff, tail));
+			skb->data = skb->head + NET_SKB_PAD;
+			skb_reset_tail_pointer(skb);
+			skb_reserve(skb, NET_SKB_PAD);
+#ifdef NET_SKBUFF_DATA_USES_OFFSET
+			skb->mac_header = ~0U;
+#endif
+			return skb;
+		}
+	}
+
+	len = (length <= SKB_RECYCLE_SIZE) ? SKB_RECYCLE_SIZE : length;
 	/*
 	 * There is more code here than it seems:
 	 * __dev_alloc_skb is an inline
 	 */
-	return __dev_alloc_skb(length, GFP_ATOMIC);
+	return __dev_alloc_skb(len, GFP_ATOMIC);
 }
 EXPORT_SYMBOL(dev_alloc_skb);
 
@@ -530,6 +592,31 @@
 }
 EXPORT_SYMBOL(kfree_skb);
 
+static inline bool consume_skb_can_recycle(const struct sk_buff *skb,
+					   int skb_size)
+{
+	if (unlikely(irqs_disabled()))
+		return false;
+
+	if (unlikely(skb_shinfo(skb)->tx_flags & SKBTX_DEV_ZEROCOPY))
+		return false;
+
+	if (unlikely(skb_is_nonlinear(skb)))
+		return false;
+
+	if (unlikely(skb->fclone != SKB_FCLONE_UNAVAILABLE))
+		return false;
+
+	skb_size = SKB_DATA_ALIGN(skb_size + NET_SKB_PAD);
+	if (unlikely(skb_end_pointer(skb) - skb->head < skb_size))
+		return false;
+
+	if (unlikely(skb_cloned(skb)))
+		return false;
+
+	return true;
+}
+
 /**
  *	consume_skb - free an skbuff
  *	@skb: buffer to free
@@ -546,8 +633,46 @@
 		smp_rmb();
 	else if (likely(!atomic_dec_and_test(&skb->users)))
 		return;
+
+	/*
+	 * If possible we'd like to recycle any skb rather than just free it,
+	 * but in order to do that we need to release any head state too.
+	 * We don't want to do this later because we'll be in a pre-emption
+	 * disabled state.
+	 */
+	skb_release_head_state(skb);
+
+	/*
+	 * Can we recycle this skb?  If we can then it will be much faster
+	 * for us to recycle this one later than to allocate a new one
+	 * from scratch.
+	 */
+	if (likely(consume_skb_can_recycle(skb, SKB_RECYCLE_SIZE))) {
+		unsigned long flags;
+		struct sk_buff_head *h;
+
+		h = &get_cpu_var(recycle_list);
+		local_irq_save(flags);
+		if (likely(skb_queue_len(h) < SKB_RECYCLE_MAX_SKBS)) {
+			__skb_queue_head(h, skb);
+			local_irq_restore(flags);
+			put_cpu_var(recycle_list);
+			return;
+		}
+
+		local_irq_restore(flags);
+		put_cpu_var(recycle_list);
+	}
+
 	trace_consume_skb(skb);
-	__kfree_skb(skb);
+
+	/*
+	 * We're not recycling so now we need to do the rest of what we would
+	 * have done in __kfree_skb (above and beyond the skb_release_head_state
+	 * that we already did.
+	 */
+	skb_release_data(skb);
+	kfree_skbmem(skb);
 }
 EXPORT_SYMBOL(consume_skb);
 
@@ -2962,8 +3087,23 @@
 }
 EXPORT_SYMBOL_GPL(skb_gro_receive);
 
+static int skb_cpu_callback(struct notifier_block *nfb,
+			    unsigned long action, void *ocpu)
+{
+	unsigned long oldcpu = (unsigned long)ocpu;
+
+	if (action == CPU_DEAD || action == CPU_DEAD_FROZEN) {
+		struct sk_buff_head *h = &per_cpu(recycle_list, oldcpu);
+		skb_queue_purge(h);
+	}
+
+	return NOTIFY_OK;
+}
+
 void __init skb_init(void)
 {
+	int cpu;
+
 	skbuff_head_cache = kmem_cache_create("skbuff_head_cache",
 					      sizeof(struct sk_buff),
 					      0,
@@ -2975,6 +3115,14 @@
 						0,
 						SLAB_HWCACHE_ALIGN|SLAB_PANIC,
 						NULL);
+
+	for_each_possible_cpu(cpu) {
+		struct sk_buff_head *h = &per_cpu(recycle_list, cpu);
+		skb_queue_head_init(h);
+		printk(KERN_INFO "Initialized recycle list for cpu %d.\n", cpu);
+	}
+
+	hotcpu_notifier(skb_cpu_callback, 0);
 }
 
 /**
